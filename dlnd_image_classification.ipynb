{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 5:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1014, 1: 1014, 2: 952, 3: 1016, 4: 997, 5: 1025, 6: 980, 7: 977, 8: 1003, 9: 1022}\n",
      "First 20 Labels: [1, 8, 5, 1, 5, 7, 4, 3, 8, 2, 7, 2, 0, 1, 5, 9, 6, 2, 0, 8]\n",
      "\n",
      "Example of Image 3:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3e713ac7b8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 5\n",
    "sample_id = 3\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [a, b]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    a = -0.5\n",
    "    b = 0.5\n",
    "    min_val = 0\n",
    "    max_val = 255\n",
    "    return a + ( ( (image_data - min_val)*(b - a) )/( max_val - min_val ) )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(np.arange(0, 10))\n",
    "    return lb.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, *image_shape], name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    input_shape = x_tensor.get_shape().as_list()\n",
    "\n",
    "    filter_weights = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], input_shape[3], conv_num_outputs], mean=0, stddev=0.1))\n",
    "    filter_bias = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    strides = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    padding = 'SAME'\n",
    "    conv = tf.nn.conv2d(x_tensor, filter_weights, strides, padding)\n",
    "    conv = tf.nn.bias_add(conv, filter_bias)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    \n",
    "    ksize = [1, pool_ksize[0], pool_ksize[1], 1]\n",
    "    strides = [1, pool_strides[0], pool_strides[1], 1]\n",
    "    padding = 'SAME'\n",
    "    return tf.nn.max_pool(conv,ksize,strides,padding=padding)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # tf.layers.dense(x_tensor, num_outputs, activation=None)\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs, activation_fn=None)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # tf.layers.dense(x_tensor, num_outputs, activation=None)\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs, activation_fn=None)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    num_outputs = 10 # [airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck]\n",
    "    conv_ksize, conv_strides = [3,3], [1,1]\n",
    "    pool_ksize, pool_strides = [2,2], [2,2]\n",
    "    \n",
    "    conv1 = conv2d_maxpool(x, 32, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv2 = conv2d_maxpool(conv1, 24, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv3 = conv2d_maxpool(conv2, 16, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    f1 = flatten(conv3)\n",
    "    \n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fc1 = fully_conn(f1, num_outputs)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out = output(fc1, num_outputs)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function \n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob:1.0})\n",
    "    validation_accuracy = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob:1.0})\n",
    "    \n",
    "    print('Current loss : {} Validation Accuracy: {}'.format(loss, validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 200\n",
    "keep_probability = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Current loss : 2.1404201984405518 Validation Accuracy: 0.23019999265670776\n",
      "Epoch  2, CIFAR-10 Batch 1:  Current loss : 1.960931420326233 Validation Accuracy: 0.3001999855041504\n",
      "Epoch  3, CIFAR-10 Batch 1:  Current loss : 1.8160004615783691 Validation Accuracy: 0.36419999599456787\n",
      "Epoch  4, CIFAR-10 Batch 1:  Current loss : 1.7508413791656494 Validation Accuracy: 0.39159998297691345\n",
      "Epoch  5, CIFAR-10 Batch 1:  Current loss : 1.6565430164337158 Validation Accuracy: 0.4187999963760376\n",
      "Epoch  6, CIFAR-10 Batch 1:  Current loss : 1.6212316751480103 Validation Accuracy: 0.41999995708465576\n",
      "Epoch  7, CIFAR-10 Batch 1:  Current loss : 1.5533783435821533 Validation Accuracy: 0.44099998474121094\n",
      "Epoch  8, CIFAR-10 Batch 1:  Current loss : 1.5518786907196045 Validation Accuracy: 0.44179999828338623\n",
      "Epoch  9, CIFAR-10 Batch 1:  Current loss : 1.49874746799469 Validation Accuracy: 0.46459996700286865\n",
      "Epoch 10, CIFAR-10 Batch 1:  Current loss : 1.4666521549224854 Validation Accuracy: 0.47759994864463806\n",
      "Epoch 11, CIFAR-10 Batch 1:  Current loss : 1.4292752742767334 Validation Accuracy: 0.49139994382858276\n",
      "Epoch 12, CIFAR-10 Batch 1:  Current loss : 1.4235910177230835 Validation Accuracy: 0.4923999309539795\n",
      "Epoch 13, CIFAR-10 Batch 1:  Current loss : 1.4200773239135742 Validation Accuracy: 0.494799941778183\n",
      "Epoch 14, CIFAR-10 Batch 1:  Current loss : 1.379530906677246 Validation Accuracy: 0.5097999572753906\n",
      "Epoch 15, CIFAR-10 Batch 1:  Current loss : 1.3886100053787231 Validation Accuracy: 0.49679994583129883\n",
      "Epoch 16, CIFAR-10 Batch 1:  Current loss : 1.3300666809082031 Validation Accuracy: 0.5203999280929565\n",
      "Epoch 17, CIFAR-10 Batch 1:  Current loss : 1.3581687211990356 Validation Accuracy: 0.49619996547698975\n",
      "Epoch 18, CIFAR-10 Batch 1:  Current loss : 1.2945761680603027 Validation Accuracy: 0.5189999341964722\n",
      "Epoch 19, CIFAR-10 Batch 1:  Current loss : 1.2913408279418945 Validation Accuracy: 0.5221999287605286\n",
      "Epoch 20, CIFAR-10 Batch 1:  Current loss : 1.283439040184021 Validation Accuracy: 0.5203999280929565\n",
      "Epoch 21, CIFAR-10 Batch 1:  Current loss : 1.2524608373641968 Validation Accuracy: 0.5287999510765076\n",
      "Epoch 22, CIFAR-10 Batch 1:  Current loss : 1.2282676696777344 Validation Accuracy: 0.5323999524116516\n",
      "Epoch 23, CIFAR-10 Batch 1:  Current loss : 1.212867021560669 Validation Accuracy: 0.5371999144554138\n",
      "Epoch 24, CIFAR-10 Batch 1:  Current loss : 1.2062358856201172 Validation Accuracy: 0.5343999862670898\n",
      "Epoch 25, CIFAR-10 Batch 1:  Current loss : 1.1782071590423584 Validation Accuracy: 0.5371999740600586\n",
      "Epoch 26, CIFAR-10 Batch 1:  Current loss : 1.1675879955291748 Validation Accuracy: 0.5399999618530273\n",
      "Epoch 27, CIFAR-10 Batch 1:  Current loss : 1.1652848720550537 Validation Accuracy: 0.5425999760627747\n",
      "Epoch 28, CIFAR-10 Batch 1:  Current loss : 1.1356130838394165 Validation Accuracy: 0.5501999258995056\n",
      "Epoch 29, CIFAR-10 Batch 1:  Current loss : 1.138736605644226 Validation Accuracy: 0.5411999225616455\n",
      "Epoch 30, CIFAR-10 Batch 1:  Current loss : 1.1113590002059937 Validation Accuracy: 0.5517999529838562\n",
      "Epoch 31, CIFAR-10 Batch 1:  Current loss : 1.1093512773513794 Validation Accuracy: 0.5431999564170837\n",
      "Epoch 32, CIFAR-10 Batch 1:  Current loss : 1.108507513999939 Validation Accuracy: 0.5541999340057373\n",
      "Epoch 33, CIFAR-10 Batch 1:  Current loss : 1.0875743627548218 Validation Accuracy: 0.5619999766349792\n",
      "Epoch 34, CIFAR-10 Batch 1:  Current loss : 1.0910513401031494 Validation Accuracy: 0.5547999143600464\n",
      "Epoch 35, CIFAR-10 Batch 1:  Current loss : 1.0885463953018188 Validation Accuracy: 0.5525999665260315\n",
      "Epoch 36, CIFAR-10 Batch 1:  Current loss : 1.0715484619140625 Validation Accuracy: 0.5609999895095825\n",
      "Epoch 37, CIFAR-10 Batch 1:  Current loss : 1.0523797273635864 Validation Accuracy: 0.567799985408783\n",
      "Epoch 38, CIFAR-10 Batch 1:  Current loss : 1.0767592191696167 Validation Accuracy: 0.5559998750686646\n",
      "Epoch 39, CIFAR-10 Batch 1:  Current loss : 1.0331004858016968 Validation Accuracy: 0.5625999569892883\n",
      "Epoch 40, CIFAR-10 Batch 1:  Current loss : 1.0291157960891724 Validation Accuracy: 0.5589998960494995\n",
      "Epoch 41, CIFAR-10 Batch 1:  Current loss : 1.016866683959961 Validation Accuracy: 0.5603999495506287\n",
      "Epoch 42, CIFAR-10 Batch 1:  Current loss : 1.0134137868881226 Validation Accuracy: 0.5695998668670654\n",
      "Epoch 43, CIFAR-10 Batch 1:  Current loss : 1.0087881088256836 Validation Accuracy: 0.5615999102592468\n",
      "Epoch 44, CIFAR-10 Batch 1:  Current loss : 1.0104384422302246 Validation Accuracy: 0.5627999305725098\n",
      "Epoch 45, CIFAR-10 Batch 1:  Current loss : 0.9976009130477905 Validation Accuracy: 0.5637999176979065\n",
      "Epoch 46, CIFAR-10 Batch 1:  Current loss : 0.9994834661483765 Validation Accuracy: 0.5661999583244324\n",
      "Epoch 47, CIFAR-10 Batch 1:  Current loss : 0.9938300251960754 Validation Accuracy: 0.5637998580932617\n",
      "Epoch 48, CIFAR-10 Batch 1:  Current loss : 0.9644387364387512 Validation Accuracy: 0.5761999487876892\n",
      "Epoch 49, CIFAR-10 Batch 1:  Current loss : 0.9731469750404358 Validation Accuracy: 0.5607999563217163\n",
      "Epoch 50, CIFAR-10 Batch 1:  Current loss : 0.9632340669631958 Validation Accuracy: 0.571199893951416\n",
      "Epoch 51, CIFAR-10 Batch 1:  Current loss : 0.9288572669029236 Validation Accuracy: 0.5753999352455139\n",
      "Epoch 52, CIFAR-10 Batch 1:  Current loss : 0.9197142124176025 Validation Accuracy: 0.579800009727478\n",
      "Epoch 53, CIFAR-10 Batch 1:  Current loss : 0.9209819436073303 Validation Accuracy: 0.572399914264679\n",
      "Epoch 54, CIFAR-10 Batch 1:  Current loss : 0.9483346343040466 Validation Accuracy: 0.5641999244689941\n",
      "Epoch 55, CIFAR-10 Batch 1:  Current loss : 0.9200437068939209 Validation Accuracy: 0.5735999345779419\n",
      "Epoch 56, CIFAR-10 Batch 1:  Current loss : 0.9261587858200073 Validation Accuracy: 0.577799916267395\n",
      "Epoch 57, CIFAR-10 Batch 1:  Current loss : 0.9097819328308105 Validation Accuracy: 0.5761999487876892\n",
      "Epoch 58, CIFAR-10 Batch 1:  Current loss : 0.9143059253692627 Validation Accuracy: 0.5829999446868896\n",
      "Epoch 59, CIFAR-10 Batch 1:  Current loss : 0.8903466463088989 Validation Accuracy: 0.5735999345779419\n",
      "Epoch 60, CIFAR-10 Batch 1:  Current loss : 0.9270176887512207 Validation Accuracy: 0.5713999271392822\n",
      "Epoch 61, CIFAR-10 Batch 1:  Current loss : 0.9870089292526245 Validation Accuracy: 0.5547999739646912\n",
      "Epoch 62, CIFAR-10 Batch 1:  Current loss : 0.9077752828598022 Validation Accuracy: 0.5765999555587769\n",
      "Epoch 63, CIFAR-10 Batch 1:  Current loss : 0.8759788870811462 Validation Accuracy: 0.5845999121665955\n",
      "Epoch 64, CIFAR-10 Batch 1:  Current loss : 0.8640506267547607 Validation Accuracy: 0.5783999562263489\n",
      "Epoch 65, CIFAR-10 Batch 1:  Current loss : 0.8669037818908691 Validation Accuracy: 0.5825998783111572\n",
      "Epoch 66, CIFAR-10 Batch 1:  Current loss : 0.8763057589530945 Validation Accuracy: 0.5819999575614929\n",
      "Epoch 67, CIFAR-10 Batch 1:  Current loss : 0.8505159616470337 Validation Accuracy: 0.5859999060630798\n",
      "Epoch 68, CIFAR-10 Batch 1:  Current loss : 0.8544848561286926 Validation Accuracy: 0.586199939250946\n",
      "Epoch 69, CIFAR-10 Batch 1:  Current loss : 0.8386510610580444 Validation Accuracy: 0.590399980545044\n",
      "Epoch 70, CIFAR-10 Batch 1:  Current loss : 0.831778883934021 Validation Accuracy: 0.593799889087677\n",
      "Epoch 71, CIFAR-10 Batch 1:  Current loss : 0.8406695127487183 Validation Accuracy: 0.5905998945236206\n",
      "Epoch 72, CIFAR-10 Batch 1:  Current loss : 0.8190510272979736 Validation Accuracy: 0.5889999270439148\n",
      "Epoch 73, CIFAR-10 Batch 1:  Current loss : 0.8077617883682251 Validation Accuracy: 0.5959998965263367\n",
      "Epoch 74, CIFAR-10 Batch 1:  Current loss : 0.8057792782783508 Validation Accuracy: 0.595599889755249\n",
      "Epoch 75, CIFAR-10 Batch 1:  Current loss : 0.7848161458969116 Validation Accuracy: 0.5959999561309814\n",
      "Epoch 76, CIFAR-10 Batch 1:  Current loss : 0.7816042304039001 Validation Accuracy: 0.5945999026298523\n",
      "Epoch 77, CIFAR-10 Batch 1:  Current loss : 0.7582575082778931 Validation Accuracy: 0.5923998951911926\n",
      "Epoch 78, CIFAR-10 Batch 1:  Current loss : 0.7665430307388306 Validation Accuracy: 0.5893999338150024\n",
      "Epoch 79, CIFAR-10 Batch 1:  Current loss : 0.7552751898765564 Validation Accuracy: 0.5957999229431152\n",
      "Epoch 80, CIFAR-10 Batch 1:  Current loss : 0.7595005035400391 Validation Accuracy: 0.5945999026298523\n",
      "Epoch 81, CIFAR-10 Batch 1:  Current loss : 0.7554182410240173 Validation Accuracy: 0.5939999222755432\n",
      "Epoch 82, CIFAR-10 Batch 1:  Current loss : 0.7542269229888916 Validation Accuracy: 0.5915999412536621\n",
      "Epoch 83, CIFAR-10 Batch 1:  Current loss : 0.7540221214294434 Validation Accuracy: 0.5881999135017395\n",
      "Epoch 84, CIFAR-10 Batch 1:  Current loss : 0.7932620644569397 Validation Accuracy: 0.5833998918533325\n",
      "Epoch 85, CIFAR-10 Batch 1:  Current loss : 0.7508374452590942 Validation Accuracy: 0.5899999737739563\n",
      "Epoch 86, CIFAR-10 Batch 1:  Current loss : 0.7381057739257812 Validation Accuracy: 0.5971999168395996\n",
      "Epoch 87, CIFAR-10 Batch 1:  Current loss : 0.7560104131698608 Validation Accuracy: 0.5945998430252075\n",
      "Epoch 88, CIFAR-10 Batch 1:  Current loss : 0.7472323179244995 Validation Accuracy: 0.5941998958587646\n",
      "Epoch 89, CIFAR-10 Batch 1:  Current loss : 0.7386046051979065 Validation Accuracy: 0.5959999561309814\n",
      "Epoch 90, CIFAR-10 Batch 1:  Current loss : 0.7453267574310303 Validation Accuracy: 0.5897999405860901\n",
      "Epoch 91, CIFAR-10 Batch 1:  Current loss : 0.735518217086792 Validation Accuracy: 0.5957998633384705\n",
      "Epoch 92, CIFAR-10 Batch 1:  Current loss : 0.7449897527694702 Validation Accuracy: 0.5915999412536621\n",
      "Epoch 93, CIFAR-10 Batch 1:  Current loss : 0.7266314625740051 Validation Accuracy: 0.5945999026298523\n",
      "Epoch 94, CIFAR-10 Batch 1:  Current loss : 0.7178974747657776 Validation Accuracy: 0.5993998646736145\n",
      "Epoch 95, CIFAR-10 Batch 1:  Current loss : 0.7117376327514648 Validation Accuracy: 0.5911999940872192\n",
      "Epoch 96, CIFAR-10 Batch 1:  Current loss : 0.7212687730789185 Validation Accuracy: 0.5935999155044556\n",
      "Epoch 97, CIFAR-10 Batch 1:  Current loss : 0.7136944532394409 Validation Accuracy: 0.5949999094009399\n",
      "Epoch 98, CIFAR-10 Batch 1:  Current loss : 0.7308631539344788 Validation Accuracy: 0.5957999229431152\n",
      "Epoch 99, CIFAR-10 Batch 1:  Current loss : 0.7057619690895081 Validation Accuracy: 0.5995998978614807\n",
      "Epoch 100, CIFAR-10 Batch 1:  Current loss : 0.7057435512542725 Validation Accuracy: 0.5965998768806458\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Current loss : 2.1939005851745605 Validation Accuracy: 0.18039998412132263\n",
      "Epoch  1, CIFAR-10 Batch 2:  Current loss : 1.873802661895752 Validation Accuracy: 0.321399986743927\n",
      "Epoch  1, CIFAR-10 Batch 3:  Current loss : 1.7009248733520508 Validation Accuracy: 0.35319995880126953\n",
      "Epoch  1, CIFAR-10 Batch 4:  Current loss : 1.617295503616333 Validation Accuracy: 0.40779998898506165\n",
      "Epoch  1, CIFAR-10 Batch 5:  Current loss : 1.6494498252868652 Validation Accuracy: 0.41419997811317444\n",
      "Epoch  2, CIFAR-10 Batch 1:  Current loss : 1.6784964799880981 Validation Accuracy: 0.4447999596595764\n",
      "Epoch  2, CIFAR-10 Batch 2:  Current loss : 1.560352087020874 Validation Accuracy: 0.43839994072914124\n",
      "Epoch  2, CIFAR-10 Batch 3:  Current loss : 1.3611189126968384 Validation Accuracy: 0.4395999610424042\n",
      "Epoch  2, CIFAR-10 Batch 4:  Current loss : 1.385043740272522 Validation Accuracy: 0.4691999554634094\n",
      "Epoch  2, CIFAR-10 Batch 5:  Current loss : 1.4706279039382935 Validation Accuracy: 0.46959996223449707\n",
      "Epoch  3, CIFAR-10 Batch 1:  Current loss : 1.5209804773330688 Validation Accuracy: 0.48579996824264526\n",
      "Epoch  3, CIFAR-10 Batch 2:  Current loss : 1.3918681144714355 Validation Accuracy: 0.48719996213912964\n",
      "Epoch  3, CIFAR-10 Batch 3:  Current loss : 1.22645902633667 Validation Accuracy: 0.4931999742984772\n",
      "Epoch  3, CIFAR-10 Batch 4:  Current loss : 1.2797505855560303 Validation Accuracy: 0.5065999627113342\n",
      "Epoch  3, CIFAR-10 Batch 5:  Current loss : 1.3027738332748413 Validation Accuracy: 0.5179999470710754\n",
      "Epoch  4, CIFAR-10 Batch 1:  Current loss : 1.4026306867599487 Validation Accuracy: 0.5295999646186829\n",
      "Epoch  4, CIFAR-10 Batch 2:  Current loss : 1.287933111190796 Validation Accuracy: 0.5327999591827393\n",
      "Epoch  4, CIFAR-10 Batch 3:  Current loss : 1.1408357620239258 Validation Accuracy: 0.5207999348640442\n",
      "Epoch  4, CIFAR-10 Batch 4:  Current loss : 1.1746201515197754 Validation Accuracy: 0.5335999727249146\n",
      "Epoch  4, CIFAR-10 Batch 5:  Current loss : 1.2507318258285522 Validation Accuracy: 0.5235999226570129\n",
      "Epoch  5, CIFAR-10 Batch 1:  Current loss : 1.3393816947937012 Validation Accuracy: 0.5471999645233154\n",
      "Epoch  5, CIFAR-10 Batch 2:  Current loss : 1.2439855337142944 Validation Accuracy: 0.5471999049186707\n",
      "Epoch  5, CIFAR-10 Batch 3:  Current loss : 1.064825177192688 Validation Accuracy: 0.5447999238967896\n",
      "Epoch  5, CIFAR-10 Batch 4:  Current loss : 1.138387680053711 Validation Accuracy: 0.5471999645233154\n",
      "Epoch  5, CIFAR-10 Batch 5:  Current loss : 1.2171167135238647 Validation Accuracy: 0.5401999354362488\n",
      "Epoch  6, CIFAR-10 Batch 1:  Current loss : 1.2818371057510376 Validation Accuracy: 0.558199942111969\n",
      "Epoch  6, CIFAR-10 Batch 2:  Current loss : 1.1968698501586914 Validation Accuracy: 0.5633999109268188\n",
      "Epoch  6, CIFAR-10 Batch 3:  Current loss : 1.0301936864852905 Validation Accuracy: 0.5591999292373657\n",
      "Epoch  6, CIFAR-10 Batch 4:  Current loss : 1.0846534967422485 Validation Accuracy: 0.5647999048233032\n",
      "Epoch  6, CIFAR-10 Batch 5:  Current loss : 1.1721278429031372 Validation Accuracy: 0.5567999482154846\n",
      "Epoch  7, CIFAR-10 Batch 1:  Current loss : 1.2570619583129883 Validation Accuracy: 0.5771999359130859\n",
      "Epoch  7, CIFAR-10 Batch 2:  Current loss : 1.186401128768921 Validation Accuracy: 0.5697999000549316\n",
      "Epoch  7, CIFAR-10 Batch 3:  Current loss : 1.0192232131958008 Validation Accuracy: 0.5725999474525452\n",
      "Epoch  7, CIFAR-10 Batch 4:  Current loss : 1.0451502799987793 Validation Accuracy: 0.5765998959541321\n",
      "Epoch  7, CIFAR-10 Batch 5:  Current loss : 1.1231029033660889 Validation Accuracy: 0.5725999474525452\n",
      "Epoch  8, CIFAR-10 Batch 1:  Current loss : 1.2337552309036255 Validation Accuracy: 0.5755999088287354\n",
      "Epoch  8, CIFAR-10 Batch 2:  Current loss : 1.1486127376556396 Validation Accuracy: 0.5837999582290649\n",
      "Epoch  8, CIFAR-10 Batch 3:  Current loss : 0.9726188778877258 Validation Accuracy: 0.5797998905181885\n",
      "Epoch  8, CIFAR-10 Batch 4:  Current loss : 1.0263440608978271 Validation Accuracy: 0.5783998966217041\n",
      "Epoch  8, CIFAR-10 Batch 5:  Current loss : 1.1139333248138428 Validation Accuracy: 0.5717999339103699\n",
      "Epoch  9, CIFAR-10 Batch 1:  Current loss : 1.172505259513855 Validation Accuracy: 0.5915999412536621\n",
      "Epoch  9, CIFAR-10 Batch 2:  Current loss : 1.1148194074630737 Validation Accuracy: 0.5923999547958374\n",
      "Epoch  9, CIFAR-10 Batch 3:  Current loss : 0.9389104247093201 Validation Accuracy: 0.5885999202728271\n",
      "Epoch  9, CIFAR-10 Batch 4:  Current loss : 1.0035417079925537 Validation Accuracy: 0.5873998999595642\n",
      "Epoch  9, CIFAR-10 Batch 5:  Current loss : 1.0693283081054688 Validation Accuracy: 0.5887998938560486\n",
      "Epoch 10, CIFAR-10 Batch 1:  Current loss : 1.1510024070739746 Validation Accuracy: 0.5951998829841614\n",
      "Epoch 10, CIFAR-10 Batch 2:  Current loss : 1.0737136602401733 Validation Accuracy: 0.6027998924255371\n",
      "Epoch 10, CIFAR-10 Batch 3:  Current loss : 0.9074586629867554 Validation Accuracy: 0.6065998673439026\n",
      "Epoch 10, CIFAR-10 Batch 4:  Current loss : 0.9826431274414062 Validation Accuracy: 0.5979998707771301\n",
      "Epoch 10, CIFAR-10 Batch 5:  Current loss : 1.045066237449646 Validation Accuracy: 0.5957999229431152\n",
      "Epoch 11, CIFAR-10 Batch 1:  Current loss : 1.1246505975723267 Validation Accuracy: 0.6089999079704285\n",
      "Epoch 11, CIFAR-10 Batch 2:  Current loss : 1.0417002439498901 Validation Accuracy: 0.6157999038696289\n",
      "Epoch 11, CIFAR-10 Batch 3:  Current loss : 0.8726595044136047 Validation Accuracy: 0.6101999282836914\n",
      "Epoch 11, CIFAR-10 Batch 4:  Current loss : 0.9451372623443604 Validation Accuracy: 0.6127999424934387\n",
      "Epoch 11, CIFAR-10 Batch 5:  Current loss : 1.036030650138855 Validation Accuracy: 0.5957999229431152\n",
      "Epoch 12, CIFAR-10 Batch 1:  Current loss : 1.098718523979187 Validation Accuracy: 0.5873998999595642\n",
      "Epoch 12, CIFAR-10 Batch 2:  Current loss : 1.0605512857437134 Validation Accuracy: 0.6061998605728149\n",
      "Epoch 12, CIFAR-10 Batch 3:  Current loss : 0.8612033724784851 Validation Accuracy: 0.6195999383926392\n",
      "Epoch 12, CIFAR-10 Batch 4:  Current loss : 0.947963535785675 Validation Accuracy: 0.6117998957633972\n",
      "Epoch 12, CIFAR-10 Batch 5:  Current loss : 1.02720308303833 Validation Accuracy: 0.6043999195098877\n",
      "Epoch 13, CIFAR-10 Batch 1:  Current loss : 1.0881388187408447 Validation Accuracy: 0.6123998761177063\n",
      "Epoch 13, CIFAR-10 Batch 2:  Current loss : 1.0243065357208252 Validation Accuracy: 0.6181999444961548\n",
      "Epoch 13, CIFAR-10 Batch 3:  Current loss : 0.8374801278114319 Validation Accuracy: 0.6285999417304993\n",
      "Epoch 13, CIFAR-10 Batch 4:  Current loss : 0.9158819317817688 Validation Accuracy: 0.6257998943328857\n",
      "Epoch 13, CIFAR-10 Batch 5:  Current loss : 0.9998889565467834 Validation Accuracy: 0.6167998909950256\n",
      "Epoch 14, CIFAR-10 Batch 1:  Current loss : 1.0434759855270386 Validation Accuracy: 0.6203998923301697\n",
      "Epoch 14, CIFAR-10 Batch 2:  Current loss : 1.0101757049560547 Validation Accuracy: 0.6267999410629272\n",
      "Epoch 14, CIFAR-10 Batch 3:  Current loss : 0.8239379525184631 Validation Accuracy: 0.6289998292922974\n",
      "Epoch 14, CIFAR-10 Batch 4:  Current loss : 0.9186958074569702 Validation Accuracy: 0.6279999017715454\n",
      "Epoch 14, CIFAR-10 Batch 5:  Current loss : 0.9668319821357727 Validation Accuracy: 0.6261999011039734\n",
      "Epoch 15, CIFAR-10 Batch 1:  Current loss : 1.0323407649993896 Validation Accuracy: 0.6261999011039734\n",
      "Epoch 15, CIFAR-10 Batch 2:  Current loss : 0.9864116907119751 Validation Accuracy: 0.6261999607086182\n",
      "Epoch 15, CIFAR-10 Batch 3:  Current loss : 0.8129538297653198 Validation Accuracy: 0.631399929523468\n",
      "Epoch 15, CIFAR-10 Batch 4:  Current loss : 0.9079692363739014 Validation Accuracy: 0.6283999085426331\n",
      "Epoch 15, CIFAR-10 Batch 5:  Current loss : 0.9595471620559692 Validation Accuracy: 0.6263998746871948\n",
      "Epoch 16, CIFAR-10 Batch 1:  Current loss : 1.026469111442566 Validation Accuracy: 0.6285999417304993\n",
      "Epoch 16, CIFAR-10 Batch 2:  Current loss : 0.9742425084114075 Validation Accuracy: 0.6353999376296997\n",
      "Epoch 16, CIFAR-10 Batch 3:  Current loss : 0.7895388007164001 Validation Accuracy: 0.6385998725891113\n",
      "Epoch 16, CIFAR-10 Batch 4:  Current loss : 0.8729797601699829 Validation Accuracy: 0.6375998854637146\n",
      "Epoch 16, CIFAR-10 Batch 5:  Current loss : 0.9481512308120728 Validation Accuracy: 0.63319993019104\n",
      "Epoch 17, CIFAR-10 Batch 1:  Current loss : 0.9828776717185974 Validation Accuracy: 0.6317999362945557\n",
      "Epoch 17, CIFAR-10 Batch 2:  Current loss : 0.9464256763458252 Validation Accuracy: 0.6403998732566833\n",
      "Epoch 17, CIFAR-10 Batch 3:  Current loss : 0.7919307351112366 Validation Accuracy: 0.6383998990058899\n",
      "Epoch 17, CIFAR-10 Batch 4:  Current loss : 0.8646535873413086 Validation Accuracy: 0.6385998725891113\n",
      "Epoch 17, CIFAR-10 Batch 5:  Current loss : 0.9175827503204346 Validation Accuracy: 0.6427999138832092\n",
      "Epoch 18, CIFAR-10 Batch 1:  Current loss : 0.9762493371963501 Validation Accuracy: 0.6423999071121216\n",
      "Epoch 18, CIFAR-10 Batch 2:  Current loss : 0.9426829814910889 Validation Accuracy: 0.6433998942375183\n",
      "Epoch 18, CIFAR-10 Batch 3:  Current loss : 0.771439254283905 Validation Accuracy: 0.6463999152183533\n",
      "Epoch 18, CIFAR-10 Batch 4:  Current loss : 0.8708946704864502 Validation Accuracy: 0.6397998332977295\n",
      "Epoch 18, CIFAR-10 Batch 5:  Current loss : 0.9141246676445007 Validation Accuracy: 0.6439998745918274\n",
      "Epoch 19, CIFAR-10 Batch 1:  Current loss : 0.9483576416969299 Validation Accuracy: 0.6441999077796936\n",
      "Epoch 19, CIFAR-10 Batch 2:  Current loss : 0.9158825874328613 Validation Accuracy: 0.6501998901367188\n",
      "Epoch 19, CIFAR-10 Batch 3:  Current loss : 0.7679075598716736 Validation Accuracy: 0.6511998772621155\n",
      "Epoch 19, CIFAR-10 Batch 4:  Current loss : 0.8370040655136108 Validation Accuracy: 0.6469998955726624\n",
      "Epoch 19, CIFAR-10 Batch 5:  Current loss : 0.8995217084884644 Validation Accuracy: 0.6517998576164246\n",
      "Epoch 20, CIFAR-10 Batch 1:  Current loss : 0.9257192611694336 Validation Accuracy: 0.6525998711585999\n",
      "Epoch 20, CIFAR-10 Batch 2:  Current loss : 0.8903281688690186 Validation Accuracy: 0.6557998657226562\n",
      "Epoch 20, CIFAR-10 Batch 3:  Current loss : 0.7649483680725098 Validation Accuracy: 0.6501998901367188\n",
      "Epoch 20, CIFAR-10 Batch 4:  Current loss : 0.8276218771934509 Validation Accuracy: 0.6525998711585999\n",
      "Epoch 20, CIFAR-10 Batch 5:  Current loss : 0.8780385255813599 Validation Accuracy: 0.6555998921394348\n",
      "Epoch 21, CIFAR-10 Batch 1:  Current loss : 0.9118562936782837 Validation Accuracy: 0.6507998704910278\n",
      "Epoch 21, CIFAR-10 Batch 2:  Current loss : 0.9149813652038574 Validation Accuracy: 0.645599901676178\n",
      "Epoch 21, CIFAR-10 Batch 3:  Current loss : 0.7410898208618164 Validation Accuracy: 0.6579998731613159\n",
      "Epoch 21, CIFAR-10 Batch 4:  Current loss : 0.8148171305656433 Validation Accuracy: 0.6543998718261719\n",
      "Epoch 21, CIFAR-10 Batch 5:  Current loss : 0.8858017921447754 Validation Accuracy: 0.6513999104499817\n",
      "Epoch 22, CIFAR-10 Batch 1:  Current loss : 0.9149585366249084 Validation Accuracy: 0.650999903678894\n",
      "Epoch 22, CIFAR-10 Batch 2:  Current loss : 0.8560728430747986 Validation Accuracy: 0.6475999355316162\n",
      "Epoch 22, CIFAR-10 Batch 3:  Current loss : 0.7179722785949707 Validation Accuracy: 0.6599999070167542\n",
      "Epoch 22, CIFAR-10 Batch 4:  Current loss : 0.8024967908859253 Validation Accuracy: 0.6579998731613159\n",
      "Epoch 22, CIFAR-10 Batch 5:  Current loss : 0.8614816665649414 Validation Accuracy: 0.662199854850769\n",
      "Epoch 23, CIFAR-10 Batch 1:  Current loss : 0.9014730453491211 Validation Accuracy: 0.6561998724937439\n",
      "Epoch 23, CIFAR-10 Batch 2:  Current loss : 0.8695048093795776 Validation Accuracy: 0.6511998176574707\n",
      "Epoch 23, CIFAR-10 Batch 3:  Current loss : 0.7389315366744995 Validation Accuracy: 0.6625999212265015\n",
      "Epoch 23, CIFAR-10 Batch 4:  Current loss : 0.8204813003540039 Validation Accuracy: 0.645599901676178\n",
      "Epoch 23, CIFAR-10 Batch 5:  Current loss : 0.8669534921646118 Validation Accuracy: 0.6629998683929443\n",
      "Epoch 24, CIFAR-10 Batch 1:  Current loss : 0.8843370079994202 Validation Accuracy: 0.653999924659729\n",
      "Epoch 24, CIFAR-10 Batch 2:  Current loss : 0.8353283405303955 Validation Accuracy: 0.6641998291015625\n",
      "Epoch 24, CIFAR-10 Batch 3:  Current loss : 0.7050259113311768 Validation Accuracy: 0.6661998629570007\n",
      "Epoch 24, CIFAR-10 Batch 4:  Current loss : 0.8037465214729309 Validation Accuracy: 0.6591998934745789\n",
      "Epoch 24, CIFAR-10 Batch 5:  Current loss : 0.8906100988388062 Validation Accuracy: 0.6609998941421509\n",
      "Epoch 25, CIFAR-10 Batch 1:  Current loss : 0.8725862503051758 Validation Accuracy: 0.6627998352050781\n",
      "Epoch 25, CIFAR-10 Batch 2:  Current loss : 0.832291841506958 Validation Accuracy: 0.6673998832702637\n",
      "Epoch 25, CIFAR-10 Batch 3:  Current loss : 0.7259007096290588 Validation Accuracy: 0.6693998575210571\n",
      "Epoch 25, CIFAR-10 Batch 4:  Current loss : 0.8088729977607727 Validation Accuracy: 0.6533998250961304\n",
      "Epoch 25, CIFAR-10 Batch 5:  Current loss : 0.8355013728141785 Validation Accuracy: 0.6639998555183411\n",
      "Epoch 26, CIFAR-10 Batch 1:  Current loss : 0.8672313690185547 Validation Accuracy: 0.6645998954772949\n",
      "Epoch 26, CIFAR-10 Batch 2:  Current loss : 0.8240985870361328 Validation Accuracy: 0.6639998555183411\n",
      "Epoch 26, CIFAR-10 Batch 3:  Current loss : 0.704670786857605 Validation Accuracy: 0.6715998649597168\n",
      "Epoch 26, CIFAR-10 Batch 4:  Current loss : 0.7986750602722168 Validation Accuracy: 0.6531998515129089\n",
      "Epoch 26, CIFAR-10 Batch 5:  Current loss : 0.8388141393661499 Validation Accuracy: 0.6665998697280884\n",
      "Epoch 27, CIFAR-10 Batch 1:  Current loss : 0.8458704948425293 Validation Accuracy: 0.6531999111175537\n",
      "Epoch 27, CIFAR-10 Batch 2:  Current loss : 0.8186399936676025 Validation Accuracy: 0.6617998480796814\n",
      "Epoch 27, CIFAR-10 Batch 3:  Current loss : 0.68885338306427 Validation Accuracy: 0.6775998473167419\n",
      "Epoch 27, CIFAR-10 Batch 4:  Current loss : 0.7714119553565979 Validation Accuracy: 0.6643998622894287\n",
      "Epoch 27, CIFAR-10 Batch 5:  Current loss : 0.8166139721870422 Validation Accuracy: 0.6681998372077942\n",
      "Epoch 28, CIFAR-10 Batch 1:  Current loss : 0.8304986357688904 Validation Accuracy: 0.6529998779296875\n",
      "Epoch 28, CIFAR-10 Batch 2:  Current loss : 0.8096197843551636 Validation Accuracy: 0.6673998832702637\n",
      "Epoch 28, CIFAR-10 Batch 3:  Current loss : 0.6778653860092163 Validation Accuracy: 0.6755998730659485\n",
      "Epoch 28, CIFAR-10 Batch 4:  Current loss : 0.7705238461494446 Validation Accuracy: 0.6713998913764954\n",
      "Epoch 28, CIFAR-10 Batch 5:  Current loss : 0.836593747138977 Validation Accuracy: 0.6711998581886292\n",
      "Epoch 29, CIFAR-10 Batch 1:  Current loss : 0.8346940279006958 Validation Accuracy: 0.6641998291015625\n",
      "Epoch 29, CIFAR-10 Batch 2:  Current loss : 0.8094237446784973 Validation Accuracy: 0.6657999157905579\n",
      "Epoch 29, CIFAR-10 Batch 3:  Current loss : 0.6846739053726196 Validation Accuracy: 0.669999897480011\n",
      "Epoch 29, CIFAR-10 Batch 4:  Current loss : 0.7631700038909912 Validation Accuracy: 0.6697998642921448\n",
      "Epoch 29, CIFAR-10 Batch 5:  Current loss : 0.8149406909942627 Validation Accuracy: 0.6705998778343201\n",
      "Epoch 30, CIFAR-10 Batch 1:  Current loss : 0.8241127729415894 Validation Accuracy: 0.6575998067855835\n",
      "Epoch 30, CIFAR-10 Batch 2:  Current loss : 0.7976753115653992 Validation Accuracy: 0.6733998656272888\n",
      "Epoch 30, CIFAR-10 Batch 3:  Current loss : 0.6649386882781982 Validation Accuracy: 0.679399847984314\n",
      "Epoch 30, CIFAR-10 Batch 4:  Current loss : 0.7908545136451721 Validation Accuracy: 0.6611998677253723\n",
      "Epoch 30, CIFAR-10 Batch 5:  Current loss : 0.812059760093689 Validation Accuracy: 0.6703999042510986\n",
      "Epoch 31, CIFAR-10 Batch 1:  Current loss : 0.8134020566940308 Validation Accuracy: 0.6601999402046204\n",
      "Epoch 31, CIFAR-10 Batch 2:  Current loss : 0.7930998206138611 Validation Accuracy: 0.6777998805046082\n",
      "Epoch 31, CIFAR-10 Batch 3:  Current loss : 0.6777948141098022 Validation Accuracy: 0.679399847984314\n",
      "Epoch 31, CIFAR-10 Batch 4:  Current loss : 0.7561708688735962 Validation Accuracy: 0.6713998913764954\n",
      "Epoch 31, CIFAR-10 Batch 5:  Current loss : 0.7950618267059326 Validation Accuracy: 0.6731998324394226\n",
      "Epoch 32, CIFAR-10 Batch 1:  Current loss : 0.8149684071540833 Validation Accuracy: 0.6675999164581299\n",
      "Epoch 32, CIFAR-10 Batch 2:  Current loss : 0.7818489074707031 Validation Accuracy: 0.6759999394416809\n",
      "Epoch 32, CIFAR-10 Batch 3:  Current loss : 0.6512144207954407 Validation Accuracy: 0.6793999075889587\n",
      "Epoch 32, CIFAR-10 Batch 4:  Current loss : 0.7601133584976196 Validation Accuracy: 0.6705998778343201\n",
      "Epoch 32, CIFAR-10 Batch 5:  Current loss : 0.7955799102783203 Validation Accuracy: 0.6809998750686646\n",
      "Epoch 33, CIFAR-10 Batch 1:  Current loss : 0.8050636053085327 Validation Accuracy: 0.6607999205589294\n",
      "Epoch 33, CIFAR-10 Batch 2:  Current loss : 0.7675769329071045 Validation Accuracy: 0.6727998852729797\n",
      "Epoch 33, CIFAR-10 Batch 3:  Current loss : 0.672049880027771 Validation Accuracy: 0.6843998432159424\n",
      "Epoch 33, CIFAR-10 Batch 4:  Current loss : 0.7673715949058533 Validation Accuracy: 0.666999876499176\n",
      "Epoch 33, CIFAR-10 Batch 5:  Current loss : 0.8030023574829102 Validation Accuracy: 0.6755999326705933\n",
      "Epoch 34, CIFAR-10 Batch 1:  Current loss : 0.7992761731147766 Validation Accuracy: 0.6637998819351196\n",
      "Epoch 34, CIFAR-10 Batch 2:  Current loss : 0.7676618099212646 Validation Accuracy: 0.6749998927116394\n",
      "Epoch 34, CIFAR-10 Batch 3:  Current loss : 0.6439557075500488 Validation Accuracy: 0.6799998879432678\n",
      "Epoch 34, CIFAR-10 Batch 4:  Current loss : 0.7534141540527344 Validation Accuracy: 0.6731998920440674\n",
      "Epoch 34, CIFAR-10 Batch 5:  Current loss : 0.7751612663269043 Validation Accuracy: 0.6809998154640198\n",
      "Epoch 35, CIFAR-10 Batch 1:  Current loss : 0.8062970042228699 Validation Accuracy: 0.6663998365402222\n",
      "Epoch 35, CIFAR-10 Batch 2:  Current loss : 0.7735908627510071 Validation Accuracy: 0.679399847984314\n",
      "Epoch 35, CIFAR-10 Batch 3:  Current loss : 0.6557350754737854 Validation Accuracy: 0.6785998940467834\n",
      "Epoch 35, CIFAR-10 Batch 4:  Current loss : 0.7524763345718384 Validation Accuracy: 0.6737999320030212\n",
      "Epoch 35, CIFAR-10 Batch 5:  Current loss : 0.7849115133285522 Validation Accuracy: 0.681199848651886\n",
      "Epoch 36, CIFAR-10 Batch 1:  Current loss : 0.7998217344284058 Validation Accuracy: 0.6625999212265015\n",
      "Epoch 36, CIFAR-10 Batch 2:  Current loss : 0.7586580514907837 Validation Accuracy: 0.6773998737335205\n",
      "Epoch 36, CIFAR-10 Batch 3:  Current loss : 0.6311531066894531 Validation Accuracy: 0.6805999279022217\n",
      "Epoch 36, CIFAR-10 Batch 4:  Current loss : 0.7502814531326294 Validation Accuracy: 0.674599826335907\n",
      "Epoch 36, CIFAR-10 Batch 5:  Current loss : 0.7837042212486267 Validation Accuracy: 0.6803998947143555\n",
      "Epoch 37, CIFAR-10 Batch 1:  Current loss : 0.7998322248458862 Validation Accuracy: 0.6777998208999634\n",
      "Epoch 37, CIFAR-10 Batch 2:  Current loss : 0.741359531879425 Validation Accuracy: 0.6779998540878296\n",
      "Epoch 37, CIFAR-10 Batch 3:  Current loss : 0.6216884255409241 Validation Accuracy: 0.6839998364448547\n",
      "Epoch 37, CIFAR-10 Batch 4:  Current loss : 0.7362862825393677 Validation Accuracy: 0.6781998872756958\n",
      "Epoch 37, CIFAR-10 Batch 5:  Current loss : 0.7695767879486084 Validation Accuracy: 0.6807999014854431\n",
      "Epoch 38, CIFAR-10 Batch 1:  Current loss : 0.7792635560035706 Validation Accuracy: 0.6757998466491699\n",
      "Epoch 38, CIFAR-10 Batch 2:  Current loss : 0.7454570531845093 Validation Accuracy: 0.6821998357772827\n",
      "Epoch 38, CIFAR-10 Batch 3:  Current loss : 0.6364250183105469 Validation Accuracy: 0.6861997842788696\n",
      "Epoch 38, CIFAR-10 Batch 4:  Current loss : 0.7249089479446411 Validation Accuracy: 0.6797998547554016\n",
      "Epoch 38, CIFAR-10 Batch 5:  Current loss : 0.7480088472366333 Validation Accuracy: 0.6909998059272766\n",
      "Epoch 39, CIFAR-10 Batch 1:  Current loss : 0.7798313498497009 Validation Accuracy: 0.679399847984314\n",
      "Epoch 39, CIFAR-10 Batch 2:  Current loss : 0.738990068435669 Validation Accuracy: 0.6765998005867004\n",
      "Epoch 39, CIFAR-10 Batch 3:  Current loss : 0.6261419653892517 Validation Accuracy: 0.6885998845100403\n",
      "Epoch 39, CIFAR-10 Batch 4:  Current loss : 0.7549045085906982 Validation Accuracy: 0.6719998717308044\n",
      "Epoch 39, CIFAR-10 Batch 5:  Current loss : 0.7476931810379028 Validation Accuracy: 0.6869998574256897\n",
      "Epoch 40, CIFAR-10 Batch 1:  Current loss : 0.7726240754127502 Validation Accuracy: 0.6811999082565308\n",
      "Epoch 40, CIFAR-10 Batch 2:  Current loss : 0.7350998520851135 Validation Accuracy: 0.6871997714042664\n",
      "Epoch 40, CIFAR-10 Batch 3:  Current loss : 0.6196874380111694 Validation Accuracy: 0.6847999095916748\n",
      "Epoch 40, CIFAR-10 Batch 4:  Current loss : 0.7287153601646423 Validation Accuracy: 0.6823998093605042\n",
      "Epoch 40, CIFAR-10 Batch 5:  Current loss : 0.7641022205352783 Validation Accuracy: 0.6817998290061951\n",
      "Epoch 41, CIFAR-10 Batch 1:  Current loss : 0.7887781858444214 Validation Accuracy: 0.674799919128418\n",
      "Epoch 41, CIFAR-10 Batch 2:  Current loss : 0.7231857180595398 Validation Accuracy: 0.6821998357772827\n",
      "Epoch 41, CIFAR-10 Batch 3:  Current loss : 0.6138823628425598 Validation Accuracy: 0.6883999109268188\n",
      "Epoch 41, CIFAR-10 Batch 4:  Current loss : 0.7564014196395874 Validation Accuracy: 0.6687999367713928\n",
      "Epoch 41, CIFAR-10 Batch 5:  Current loss : 0.7559623718261719 Validation Accuracy: 0.6909998655319214\n",
      "Epoch 42, CIFAR-10 Batch 1:  Current loss : 0.786467432975769 Validation Accuracy: 0.6883999109268188\n",
      "Epoch 42, CIFAR-10 Batch 2:  Current loss : 0.7192386984825134 Validation Accuracy: 0.6901998519897461\n",
      "Epoch 42, CIFAR-10 Batch 3:  Current loss : 0.6166440844535828 Validation Accuracy: 0.6911998391151428\n",
      "Epoch 42, CIFAR-10 Batch 4:  Current loss : 0.7351484298706055 Validation Accuracy: 0.6827998757362366\n",
      "Epoch 42, CIFAR-10 Batch 5:  Current loss : 0.7560954093933105 Validation Accuracy: 0.689599871635437\n",
      "Epoch 43, CIFAR-10 Batch 1:  Current loss : 0.7602498531341553 Validation Accuracy: 0.6867998838424683\n",
      "Epoch 43, CIFAR-10 Batch 2:  Current loss : 0.7376834154129028 Validation Accuracy: 0.6883997917175293\n",
      "Epoch 43, CIFAR-10 Batch 3:  Current loss : 0.6089425683021545 Validation Accuracy: 0.6879998445510864\n",
      "Epoch 43, CIFAR-10 Batch 4:  Current loss : 0.7130275964736938 Validation Accuracy: 0.6819998621940613\n",
      "Epoch 43, CIFAR-10 Batch 5:  Current loss : 0.758870005607605 Validation Accuracy: 0.6919999122619629\n",
      "Epoch 44, CIFAR-10 Batch 1:  Current loss : 0.7649831175804138 Validation Accuracy: 0.6837998628616333\n",
      "Epoch 44, CIFAR-10 Batch 2:  Current loss : 0.710472047328949 Validation Accuracy: 0.6865999102592468\n",
      "Epoch 44, CIFAR-10 Batch 3:  Current loss : 0.6339905858039856 Validation Accuracy: 0.6871998906135559\n",
      "Epoch 44, CIFAR-10 Batch 4:  Current loss : 0.7109887003898621 Validation Accuracy: 0.6835999488830566\n",
      "Epoch 44, CIFAR-10 Batch 5:  Current loss : 0.7348861694335938 Validation Accuracy: 0.6963999271392822\n",
      "Epoch 45, CIFAR-10 Batch 1:  Current loss : 0.762236475944519 Validation Accuracy: 0.6855998635292053\n",
      "Epoch 45, CIFAR-10 Batch 2:  Current loss : 0.7138963937759399 Validation Accuracy: 0.6863999366760254\n",
      "Epoch 45, CIFAR-10 Batch 3:  Current loss : 0.602990448474884 Validation Accuracy: 0.6907998323440552\n",
      "Epoch 45, CIFAR-10 Batch 4:  Current loss : 0.7119219899177551 Validation Accuracy: 0.6871998310089111\n",
      "Epoch 45, CIFAR-10 Batch 5:  Current loss : 0.7373459339141846 Validation Accuracy: 0.6917998790740967\n",
      "Epoch 46, CIFAR-10 Batch 1:  Current loss : 0.7643793821334839 Validation Accuracy: 0.6869997978210449\n",
      "Epoch 46, CIFAR-10 Batch 2:  Current loss : 0.715441882610321 Validation Accuracy: 0.6883997917175293\n",
      "Epoch 46, CIFAR-10 Batch 3:  Current loss : 0.5931593179702759 Validation Accuracy: 0.6951999068260193\n",
      "Epoch 46, CIFAR-10 Batch 4:  Current loss : 0.7239086627960205 Validation Accuracy: 0.6813998222351074\n",
      "Epoch 46, CIFAR-10 Batch 5:  Current loss : 0.7529528141021729 Validation Accuracy: 0.6907998323440552\n",
      "Epoch 47, CIFAR-10 Batch 1:  Current loss : 0.7644970417022705 Validation Accuracy: 0.6855998039245605\n",
      "Epoch 47, CIFAR-10 Batch 2:  Current loss : 0.7060345411300659 Validation Accuracy: 0.6841998100280762\n",
      "Epoch 47, CIFAR-10 Batch 3:  Current loss : 0.595613956451416 Validation Accuracy: 0.6885999441146851\n",
      "Epoch 47, CIFAR-10 Batch 4:  Current loss : 0.6897016167640686 Validation Accuracy: 0.6887998580932617\n",
      "Epoch 47, CIFAR-10 Batch 5:  Current loss : 0.732938826084137 Validation Accuracy: 0.694199800491333\n",
      "Epoch 48, CIFAR-10 Batch 1:  Current loss : 0.7692315578460693 Validation Accuracy: 0.6873998641967773\n",
      "Epoch 48, CIFAR-10 Batch 2:  Current loss : 0.6822245717048645 Validation Accuracy: 0.6861999034881592\n",
      "Epoch 48, CIFAR-10 Batch 3:  Current loss : 0.5891316533088684 Validation Accuracy: 0.6889998316764832\n",
      "Epoch 48, CIFAR-10 Batch 4:  Current loss : 0.7016903758049011 Validation Accuracy: 0.6855998635292053\n",
      "Epoch 48, CIFAR-10 Batch 5:  Current loss : 0.7401713132858276 Validation Accuracy: 0.6927998065948486\n",
      "Epoch 49, CIFAR-10 Batch 1:  Current loss : 0.7540028095245361 Validation Accuracy: 0.6939999461174011\n",
      "Epoch 49, CIFAR-10 Batch 2:  Current loss : 0.6892269849777222 Validation Accuracy: 0.6887999176979065\n",
      "Epoch 49, CIFAR-10 Batch 3:  Current loss : 0.5822277665138245 Validation Accuracy: 0.6969998478889465\n",
      "Epoch 49, CIFAR-10 Batch 4:  Current loss : 0.7126274108886719 Validation Accuracy: 0.6815999150276184\n",
      "Epoch 49, CIFAR-10 Batch 5:  Current loss : 0.7284194231033325 Validation Accuracy: 0.692599892616272\n",
      "Epoch 50, CIFAR-10 Batch 1:  Current loss : 0.7738668918609619 Validation Accuracy: 0.7021998167037964\n",
      "Epoch 50, CIFAR-10 Batch 2:  Current loss : 0.7040362358093262 Validation Accuracy: 0.6937998533248901\n",
      "Epoch 50, CIFAR-10 Batch 3:  Current loss : 0.5852987170219421 Validation Accuracy: 0.697999894618988\n",
      "Epoch 50, CIFAR-10 Batch 4:  Current loss : 0.7071952819824219 Validation Accuracy: 0.682999849319458\n",
      "Epoch 50, CIFAR-10 Batch 5:  Current loss : 0.751944899559021 Validation Accuracy: 0.6957999467849731\n",
      "Epoch 51, CIFAR-10 Batch 1:  Current loss : 0.751258909702301 Validation Accuracy: 0.6851998567581177\n",
      "Epoch 51, CIFAR-10 Batch 2:  Current loss : 0.7003214955329895 Validation Accuracy: 0.6949998140335083\n",
      "Epoch 51, CIFAR-10 Batch 3:  Current loss : 0.5920478105545044 Validation Accuracy: 0.6915997862815857\n",
      "Epoch 51, CIFAR-10 Batch 4:  Current loss : 0.6886482238769531 Validation Accuracy: 0.6921998262405396\n",
      "Epoch 51, CIFAR-10 Batch 5:  Current loss : 0.7466079592704773 Validation Accuracy: 0.6899998188018799\n",
      "Epoch 52, CIFAR-10 Batch 1:  Current loss : 0.7535949349403381 Validation Accuracy: 0.6929998397827148\n",
      "Epoch 52, CIFAR-10 Batch 2:  Current loss : 0.68513423204422 Validation Accuracy: 0.6955999732017517\n",
      "Epoch 52, CIFAR-10 Batch 3:  Current loss : 0.5728607177734375 Validation Accuracy: 0.6969998478889465\n",
      "Epoch 52, CIFAR-10 Batch 4:  Current loss : 0.6915093064308167 Validation Accuracy: 0.6935998797416687\n",
      "Epoch 52, CIFAR-10 Batch 5:  Current loss : 0.72159743309021 Validation Accuracy: 0.6965999007225037\n",
      "Epoch 53, CIFAR-10 Batch 1:  Current loss : 0.7526893615722656 Validation Accuracy: 0.6857998371124268\n",
      "Epoch 53, CIFAR-10 Batch 2:  Current loss : 0.7041987776756287 Validation Accuracy: 0.6935998201370239\n",
      "Epoch 53, CIFAR-10 Batch 3:  Current loss : 0.5875296592712402 Validation Accuracy: 0.6867998838424683\n",
      "Epoch 53, CIFAR-10 Batch 4:  Current loss : 0.6879330277442932 Validation Accuracy: 0.6957998275756836\n",
      "Epoch 53, CIFAR-10 Batch 5:  Current loss : 0.7459492087364197 Validation Accuracy: 0.6933998465538025\n",
      "Epoch 54, CIFAR-10 Batch 1:  Current loss : 0.7558706998825073 Validation Accuracy: 0.6843998432159424\n",
      "Epoch 54, CIFAR-10 Batch 2:  Current loss : 0.6807810664176941 Validation Accuracy: 0.7009998559951782\n",
      "Epoch 54, CIFAR-10 Batch 3:  Current loss : 0.5963656902313232 Validation Accuracy: 0.685999870300293\n",
      "Epoch 54, CIFAR-10 Batch 4:  Current loss : 0.6686573028564453 Validation Accuracy: 0.6925998330116272\n",
      "Epoch 54, CIFAR-10 Batch 5:  Current loss : 0.7537830471992493 Validation Accuracy: 0.6887999773025513\n",
      "Epoch 55, CIFAR-10 Batch 1:  Current loss : 0.7554334402084351 Validation Accuracy: 0.6845998167991638\n",
      "Epoch 55, CIFAR-10 Batch 2:  Current loss : 0.6873762607574463 Validation Accuracy: 0.6949998140335083\n",
      "Epoch 55, CIFAR-10 Batch 3:  Current loss : 0.5924113392829895 Validation Accuracy: 0.682999849319458\n",
      "Epoch 55, CIFAR-10 Batch 4:  Current loss : 0.6974943280220032 Validation Accuracy: 0.6853998899459839\n",
      "Epoch 55, CIFAR-10 Batch 5:  Current loss : 0.7311369776725769 Validation Accuracy: 0.6969999074935913\n",
      "Epoch 56, CIFAR-10 Batch 1:  Current loss : 0.7556410431861877 Validation Accuracy: 0.6721999049186707\n",
      "Epoch 56, CIFAR-10 Batch 2:  Current loss : 0.6868341565132141 Validation Accuracy: 0.7003999352455139\n",
      "Epoch 56, CIFAR-10 Batch 3:  Current loss : 0.6173163652420044 Validation Accuracy: 0.6881998181343079\n",
      "Epoch 56, CIFAR-10 Batch 4:  Current loss : 0.671432375907898 Validation Accuracy: 0.6957998871803284\n",
      "Epoch 56, CIFAR-10 Batch 5:  Current loss : 0.7497548460960388 Validation Accuracy: 0.6919999122619629\n",
      "Epoch 57, CIFAR-10 Batch 1:  Current loss : 0.743250846862793 Validation Accuracy: 0.681199848651886\n",
      "Epoch 57, CIFAR-10 Batch 2:  Current loss : 0.6807453632354736 Validation Accuracy: 0.6985998749732971\n",
      "Epoch 57, CIFAR-10 Batch 3:  Current loss : 0.5841580033302307 Validation Accuracy: 0.6939998865127563\n",
      "Epoch 57, CIFAR-10 Batch 4:  Current loss : 0.6770579814910889 Validation Accuracy: 0.6877999305725098\n",
      "Epoch 57, CIFAR-10 Batch 5:  Current loss : 0.7182193398475647 Validation Accuracy: 0.6991998553276062\n",
      "Epoch 58, CIFAR-10 Batch 1:  Current loss : 0.7390916347503662 Validation Accuracy: 0.6885998249053955\n",
      "Epoch 58, CIFAR-10 Batch 2:  Current loss : 0.6711169481277466 Validation Accuracy: 0.7013998031616211\n",
      "Epoch 58, CIFAR-10 Batch 3:  Current loss : 0.5858151316642761 Validation Accuracy: 0.6897999048233032\n",
      "Epoch 58, CIFAR-10 Batch 4:  Current loss : 0.6678297519683838 Validation Accuracy: 0.6963998079299927\n",
      "Epoch 58, CIFAR-10 Batch 5:  Current loss : 0.7105096578598022 Validation Accuracy: 0.6953999400138855\n",
      "Epoch 59, CIFAR-10 Batch 1:  Current loss : 0.7404210567474365 Validation Accuracy: 0.6843998432159424\n",
      "Epoch 59, CIFAR-10 Batch 2:  Current loss : 0.6725130677223206 Validation Accuracy: 0.6973998546600342\n",
      "Epoch 59, CIFAR-10 Batch 3:  Current loss : 0.5771117806434631 Validation Accuracy: 0.6891999244689941\n",
      "Epoch 59, CIFAR-10 Batch 4:  Current loss : 0.6538439989089966 Validation Accuracy: 0.6951998472213745\n",
      "Epoch 59, CIFAR-10 Batch 5:  Current loss : 0.6952682733535767 Validation Accuracy: 0.6981998682022095\n",
      "Epoch 60, CIFAR-10 Batch 1:  Current loss : 0.7440651655197144 Validation Accuracy: 0.6899999380111694\n",
      "Epoch 60, CIFAR-10 Batch 2:  Current loss : 0.6752477288246155 Validation Accuracy: 0.7069998383522034\n",
      "Epoch 60, CIFAR-10 Batch 3:  Current loss : 0.5931367874145508 Validation Accuracy: 0.6867998242378235\n",
      "Epoch 60, CIFAR-10 Batch 4:  Current loss : 0.6628087759017944 Validation Accuracy: 0.6909999251365662\n",
      "Epoch 60, CIFAR-10 Batch 5:  Current loss : 0.7116810083389282 Validation Accuracy: 0.6961998343467712\n",
      "Epoch 61, CIFAR-10 Batch 1:  Current loss : 0.7550030946731567 Validation Accuracy: 0.6805998682975769\n",
      "Epoch 61, CIFAR-10 Batch 2:  Current loss : 0.6596770882606506 Validation Accuracy: 0.7045998573303223\n",
      "Epoch 61, CIFAR-10 Batch 3:  Current loss : 0.5651389360427856 Validation Accuracy: 0.6917998790740967\n",
      "Epoch 61, CIFAR-10 Batch 4:  Current loss : 0.6496437191963196 Validation Accuracy: 0.6911998987197876\n",
      "Epoch 61, CIFAR-10 Batch 5:  Current loss : 0.7041070461273193 Validation Accuracy: 0.700799822807312\n",
      "Epoch 62, CIFAR-10 Batch 1:  Current loss : 0.7640048265457153 Validation Accuracy: 0.6837999224662781\n",
      "Epoch 62, CIFAR-10 Batch 2:  Current loss : 0.6552923917770386 Validation Accuracy: 0.7011998891830444\n",
      "Epoch 62, CIFAR-10 Batch 3:  Current loss : 0.5669010281562805 Validation Accuracy: 0.6863999366760254\n",
      "Epoch 62, CIFAR-10 Batch 4:  Current loss : 0.656542181968689 Validation Accuracy: 0.6975998282432556\n",
      "Epoch 62, CIFAR-10 Batch 5:  Current loss : 0.7029579877853394 Validation Accuracy: 0.6997998952865601\n",
      "Epoch 63, CIFAR-10 Batch 1:  Current loss : 0.7323884963989258 Validation Accuracy: 0.6949998736381531\n",
      "Epoch 63, CIFAR-10 Batch 2:  Current loss : 0.662211000919342 Validation Accuracy: 0.703799843788147\n",
      "Epoch 63, CIFAR-10 Batch 3:  Current loss : 0.5728261470794678 Validation Accuracy: 0.6903998851776123\n",
      "Epoch 63, CIFAR-10 Batch 4:  Current loss : 0.6406804323196411 Validation Accuracy: 0.6889998316764832\n",
      "Epoch 63, CIFAR-10 Batch 5:  Current loss : 0.7127077579498291 Validation Accuracy: 0.6953998804092407\n",
      "Epoch 64, CIFAR-10 Batch 1:  Current loss : 0.7554419636726379 Validation Accuracy: 0.6789998412132263\n",
      "Epoch 64, CIFAR-10 Batch 2:  Current loss : 0.6691474914550781 Validation Accuracy: 0.6987999081611633\n",
      "Epoch 64, CIFAR-10 Batch 3:  Current loss : 0.5765405893325806 Validation Accuracy: 0.6915999054908752\n",
      "Epoch 64, CIFAR-10 Batch 4:  Current loss : 0.6603675484657288 Validation Accuracy: 0.6893998980522156\n",
      "Epoch 64, CIFAR-10 Batch 5:  Current loss : 0.6839617490768433 Validation Accuracy: 0.6977999210357666\n",
      "Epoch 65, CIFAR-10 Batch 1:  Current loss : 0.7488533854484558 Validation Accuracy: 0.6967998743057251\n",
      "Epoch 65, CIFAR-10 Batch 2:  Current loss : 0.6682483553886414 Validation Accuracy: 0.7011998891830444\n",
      "Epoch 65, CIFAR-10 Batch 3:  Current loss : 0.5728740096092224 Validation Accuracy: 0.6909998059272766\n",
      "Epoch 65, CIFAR-10 Batch 4:  Current loss : 0.6381422281265259 Validation Accuracy: 0.6939998865127563\n",
      "Epoch 65, CIFAR-10 Batch 5:  Current loss : 0.6894887685775757 Validation Accuracy: 0.7029998898506165\n",
      "Epoch 66, CIFAR-10 Batch 1:  Current loss : 0.7395097017288208 Validation Accuracy: 0.6899998188018799\n",
      "Epoch 66, CIFAR-10 Batch 2:  Current loss : 0.6585076451301575 Validation Accuracy: 0.6971998810768127\n",
      "Epoch 66, CIFAR-10 Batch 3:  Current loss : 0.579595685005188 Validation Accuracy: 0.687799870967865\n",
      "Epoch 66, CIFAR-10 Batch 4:  Current loss : 0.6477972269058228 Validation Accuracy: 0.6981998085975647\n",
      "Epoch 66, CIFAR-10 Batch 5:  Current loss : 0.6985942125320435 Validation Accuracy: 0.6991998553276062\n",
      "Epoch 67, CIFAR-10 Batch 1:  Current loss : 0.7417192459106445 Validation Accuracy: 0.6849998235702515\n",
      "Epoch 67, CIFAR-10 Batch 2:  Current loss : 0.640661895275116 Validation Accuracy: 0.7023998498916626\n",
      "Epoch 67, CIFAR-10 Batch 3:  Current loss : 0.577081024646759 Validation Accuracy: 0.684199869632721\n",
      "Epoch 67, CIFAR-10 Batch 4:  Current loss : 0.6279677748680115 Validation Accuracy: 0.6957998275756836\n",
      "Epoch 67, CIFAR-10 Batch 5:  Current loss : 0.6975719332695007 Validation Accuracy: 0.7039998173713684\n",
      "Epoch 68, CIFAR-10 Batch 1:  Current loss : 0.7370597124099731 Validation Accuracy: 0.6907998323440552\n",
      "Epoch 68, CIFAR-10 Batch 2:  Current loss : 0.6349653601646423 Validation Accuracy: 0.7021998167037964\n",
      "Epoch 68, CIFAR-10 Batch 3:  Current loss : 0.5652652382850647 Validation Accuracy: 0.6891999244689941\n",
      "Epoch 68, CIFAR-10 Batch 4:  Current loss : 0.6325440406799316 Validation Accuracy: 0.6945998668670654\n",
      "Epoch 68, CIFAR-10 Batch 5:  Current loss : 0.6967383027076721 Validation Accuracy: 0.6965998411178589\n",
      "Epoch 69, CIFAR-10 Batch 1:  Current loss : 0.7417510747909546 Validation Accuracy: 0.6901998519897461\n",
      "Epoch 69, CIFAR-10 Batch 2:  Current loss : 0.6733603477478027 Validation Accuracy: 0.695999801158905\n",
      "Epoch 69, CIFAR-10 Batch 3:  Current loss : 0.5613471269607544 Validation Accuracy: 0.691399872303009\n",
      "Epoch 69, CIFAR-10 Batch 4:  Current loss : 0.6432848572731018 Validation Accuracy: 0.694399893283844\n",
      "Epoch 69, CIFAR-10 Batch 5:  Current loss : 0.675315797328949 Validation Accuracy: 0.6989998817443848\n",
      "Epoch 70, CIFAR-10 Batch 1:  Current loss : 0.718602180480957 Validation Accuracy: 0.6839998960494995\n",
      "Epoch 70, CIFAR-10 Batch 2:  Current loss : 0.6596934199333191 Validation Accuracy: 0.6985998153686523\n",
      "Epoch 70, CIFAR-10 Batch 3:  Current loss : 0.5708349943161011 Validation Accuracy: 0.6963998675346375\n",
      "Epoch 70, CIFAR-10 Batch 4:  Current loss : 0.63202303647995 Validation Accuracy: 0.6909998655319214\n",
      "Epoch 70, CIFAR-10 Batch 5:  Current loss : 0.6876170635223389 Validation Accuracy: 0.7021998167037964\n",
      "Epoch 71, CIFAR-10 Batch 1:  Current loss : 0.7310213446617126 Validation Accuracy: 0.6901998519897461\n",
      "Epoch 71, CIFAR-10 Batch 2:  Current loss : 0.674271821975708 Validation Accuracy: 0.6961998343467712\n",
      "Epoch 71, CIFAR-10 Batch 3:  Current loss : 0.5496845245361328 Validation Accuracy: 0.6873998641967773\n",
      "Epoch 71, CIFAR-10 Batch 4:  Current loss : 0.6408512592315674 Validation Accuracy: 0.7017998695373535\n",
      "Epoch 71, CIFAR-10 Batch 5:  Current loss : 0.6797906756401062 Validation Accuracy: 0.6993998289108276\n",
      "Epoch 72, CIFAR-10 Batch 1:  Current loss : 0.7331732511520386 Validation Accuracy: 0.6909998059272766\n",
      "Epoch 72, CIFAR-10 Batch 2:  Current loss : 0.6516426205635071 Validation Accuracy: 0.6993998885154724\n",
      "Epoch 72, CIFAR-10 Batch 3:  Current loss : 0.5432341694831848 Validation Accuracy: 0.6945998072624207\n",
      "Epoch 72, CIFAR-10 Batch 4:  Current loss : 0.6387117505073547 Validation Accuracy: 0.6935999393463135\n",
      "Epoch 72, CIFAR-10 Batch 5:  Current loss : 0.7200286388397217 Validation Accuracy: 0.6929998993873596\n",
      "Epoch 73, CIFAR-10 Batch 1:  Current loss : 0.736376941204071 Validation Accuracy: 0.6993998885154724\n",
      "Epoch 73, CIFAR-10 Batch 2:  Current loss : 0.6397150158882141 Validation Accuracy: 0.708599865436554\n",
      "Epoch 73, CIFAR-10 Batch 3:  Current loss : 0.5337207317352295 Validation Accuracy: 0.6939997673034668\n",
      "Epoch 73, CIFAR-10 Batch 4:  Current loss : 0.6183962821960449 Validation Accuracy: 0.703799843788147\n",
      "Epoch 73, CIFAR-10 Batch 5:  Current loss : 0.6904162168502808 Validation Accuracy: 0.6985998153686523\n",
      "Epoch 74, CIFAR-10 Batch 1:  Current loss : 0.7306653261184692 Validation Accuracy: 0.7013998031616211\n",
      "Epoch 74, CIFAR-10 Batch 2:  Current loss : 0.6414153575897217 Validation Accuracy: 0.707399845123291\n",
      "Epoch 74, CIFAR-10 Batch 3:  Current loss : 0.5340661406517029 Validation Accuracy: 0.7015998959541321\n",
      "Epoch 74, CIFAR-10 Batch 4:  Current loss : 0.6219505071640015 Validation Accuracy: 0.7025998830795288\n",
      "Epoch 74, CIFAR-10 Batch 5:  Current loss : 0.6649799346923828 Validation Accuracy: 0.7069998383522034\n",
      "Epoch 75, CIFAR-10 Batch 1:  Current loss : 0.7331044673919678 Validation Accuracy: 0.689599871635437\n",
      "Epoch 75, CIFAR-10 Batch 2:  Current loss : 0.6380884051322937 Validation Accuracy: 0.7063998579978943\n",
      "Epoch 75, CIFAR-10 Batch 3:  Current loss : 0.5466658473014832 Validation Accuracy: 0.6939998269081116\n",
      "Epoch 75, CIFAR-10 Batch 4:  Current loss : 0.6379161477088928 Validation Accuracy: 0.6887999176979065\n",
      "Epoch 75, CIFAR-10 Batch 5:  Current loss : 0.6627594828605652 Validation Accuracy: 0.7003999352455139\n",
      "Epoch 76, CIFAR-10 Batch 1:  Current loss : 0.7265720963478088 Validation Accuracy: 0.6879998445510864\n",
      "Epoch 76, CIFAR-10 Batch 2:  Current loss : 0.635189414024353 Validation Accuracy: 0.7055999040603638\n",
      "Epoch 76, CIFAR-10 Batch 3:  Current loss : 0.526468813419342 Validation Accuracy: 0.6957999467849731\n",
      "Epoch 76, CIFAR-10 Batch 4:  Current loss : 0.6147672533988953 Validation Accuracy: 0.700799822807312\n",
      "Epoch 76, CIFAR-10 Batch 5:  Current loss : 0.7025755643844604 Validation Accuracy: 0.6937998533248901\n",
      "Epoch 77, CIFAR-10 Batch 1:  Current loss : 0.7289980053901672 Validation Accuracy: 0.6933999061584473\n",
      "Epoch 77, CIFAR-10 Batch 2:  Current loss : 0.6439146995544434 Validation Accuracy: 0.6957998871803284\n",
      "Epoch 77, CIFAR-10 Batch 3:  Current loss : 0.5328066349029541 Validation Accuracy: 0.6967998743057251\n",
      "Epoch 77, CIFAR-10 Batch 4:  Current loss : 0.6188308596611023 Validation Accuracy: 0.7029998898506165\n",
      "Epoch 77, CIFAR-10 Batch 5:  Current loss : 0.6744743585586548 Validation Accuracy: 0.7051998376846313\n",
      "Epoch 78, CIFAR-10 Batch 1:  Current loss : 0.7185181975364685 Validation Accuracy: 0.692599892616272\n",
      "Epoch 78, CIFAR-10 Batch 2:  Current loss : 0.628387451171875 Validation Accuracy: 0.703999936580658\n",
      "Epoch 78, CIFAR-10 Batch 3:  Current loss : 0.5329092741012573 Validation Accuracy: 0.6969997882843018\n",
      "Epoch 78, CIFAR-10 Batch 4:  Current loss : 0.644492506980896 Validation Accuracy: 0.693199872970581\n",
      "Epoch 78, CIFAR-10 Batch 5:  Current loss : 0.6735941171646118 Validation Accuracy: 0.7015998363494873\n",
      "Epoch 79, CIFAR-10 Batch 1:  Current loss : 0.7283406257629395 Validation Accuracy: 0.6963999271392822\n",
      "Epoch 79, CIFAR-10 Batch 2:  Current loss : 0.6184337735176086 Validation Accuracy: 0.7087998390197754\n",
      "Epoch 79, CIFAR-10 Batch 3:  Current loss : 0.5482513904571533 Validation Accuracy: 0.6967999339103699\n",
      "Epoch 79, CIFAR-10 Batch 4:  Current loss : 0.6461712121963501 Validation Accuracy: 0.6945998668670654\n",
      "Epoch 79, CIFAR-10 Batch 5:  Current loss : 0.6838616728782654 Validation Accuracy: 0.6991997957229614\n",
      "Epoch 80, CIFAR-10 Batch 1:  Current loss : 0.7244278192520142 Validation Accuracy: 0.6911998987197876\n",
      "Epoch 80, CIFAR-10 Batch 2:  Current loss : 0.6449840068817139 Validation Accuracy: 0.7077997922897339\n",
      "Epoch 80, CIFAR-10 Batch 3:  Current loss : 0.520375669002533 Validation Accuracy: 0.6949998140335083\n",
      "Epoch 80, CIFAR-10 Batch 4:  Current loss : 0.6486458778381348 Validation Accuracy: 0.7015998959541321\n",
      "Epoch 80, CIFAR-10 Batch 5:  Current loss : 0.6981443166732788 Validation Accuracy: 0.6915999054908752\n",
      "Epoch 81, CIFAR-10 Batch 1:  Current loss : 0.7224106192588806 Validation Accuracy: 0.6949998736381531\n",
      "Epoch 81, CIFAR-10 Batch 2:  Current loss : 0.6475022435188293 Validation Accuracy: 0.6977998614311218\n",
      "Epoch 81, CIFAR-10 Batch 3:  Current loss : 0.548463761806488 Validation Accuracy: 0.6919998526573181\n",
      "Epoch 81, CIFAR-10 Batch 4:  Current loss : 0.6323561072349548 Validation Accuracy: 0.6963998675346375\n",
      "Epoch 81, CIFAR-10 Batch 5:  Current loss : 0.6849677562713623 Validation Accuracy: 0.6923999190330505\n",
      "Epoch 82, CIFAR-10 Batch 1:  Current loss : 0.7234911918640137 Validation Accuracy: 0.6933999061584473\n",
      "Epoch 82, CIFAR-10 Batch 2:  Current loss : 0.6268399953842163 Validation Accuracy: 0.7053998708724976\n",
      "Epoch 82, CIFAR-10 Batch 3:  Current loss : 0.5368009209632874 Validation Accuracy: 0.6871998906135559\n",
      "Epoch 82, CIFAR-10 Batch 4:  Current loss : 0.6115288138389587 Validation Accuracy: 0.7013998627662659\n",
      "Epoch 82, CIFAR-10 Batch 5:  Current loss : 0.6651172041893005 Validation Accuracy: 0.694399893283844\n",
      "Epoch 83, CIFAR-10 Batch 1:  Current loss : 0.7125174403190613 Validation Accuracy: 0.6971998810768127\n",
      "Epoch 83, CIFAR-10 Batch 2:  Current loss : 0.6295739412307739 Validation Accuracy: 0.6973998546600342\n",
      "Epoch 83, CIFAR-10 Batch 3:  Current loss : 0.522566020488739 Validation Accuracy: 0.692599892616272\n",
      "Epoch 83, CIFAR-10 Batch 4:  Current loss : 0.610775351524353 Validation Accuracy: 0.6995998620986938\n",
      "Epoch 83, CIFAR-10 Batch 5:  Current loss : 0.6708083152770996 Validation Accuracy: 0.7005999088287354\n",
      "Epoch 84, CIFAR-10 Batch 1:  Current loss : 0.7127969861030579 Validation Accuracy: 0.697999894618988\n",
      "Epoch 84, CIFAR-10 Batch 2:  Current loss : 0.6187505125999451 Validation Accuracy: 0.7077998518943787\n",
      "Epoch 84, CIFAR-10 Batch 3:  Current loss : 0.4983294904232025 Validation Accuracy: 0.6995999217033386\n",
      "Epoch 84, CIFAR-10 Batch 4:  Current loss : 0.623841404914856 Validation Accuracy: 0.7049998641014099\n",
      "Epoch 84, CIFAR-10 Batch 5:  Current loss : 0.6722635626792908 Validation Accuracy: 0.6975998878479004\n",
      "Epoch 85, CIFAR-10 Batch 1:  Current loss : 0.702329695224762 Validation Accuracy: 0.6961998343467712\n",
      "Epoch 85, CIFAR-10 Batch 2:  Current loss : 0.6155141592025757 Validation Accuracy: 0.7039998173713684\n",
      "Epoch 85, CIFAR-10 Batch 3:  Current loss : 0.48606184124946594 Validation Accuracy: 0.7053998708724976\n",
      "Epoch 85, CIFAR-10 Batch 4:  Current loss : 0.6108675599098206 Validation Accuracy: 0.7027998566627502\n",
      "Epoch 85, CIFAR-10 Batch 5:  Current loss : 0.6601114273071289 Validation Accuracy: 0.6993998289108276\n",
      "Epoch 86, CIFAR-10 Batch 1:  Current loss : 0.7193909883499146 Validation Accuracy: 0.6977998614311218\n",
      "Epoch 86, CIFAR-10 Batch 2:  Current loss : 0.6332384347915649 Validation Accuracy: 0.7071998715400696\n",
      "Epoch 86, CIFAR-10 Batch 3:  Current loss : 0.5226855278015137 Validation Accuracy: 0.6979998350143433\n",
      "Epoch 86, CIFAR-10 Batch 4:  Current loss : 0.6135684251785278 Validation Accuracy: 0.7065998315811157\n",
      "Epoch 86, CIFAR-10 Batch 5:  Current loss : 0.662682294845581 Validation Accuracy: 0.6973998546600342\n",
      "Epoch 87, CIFAR-10 Batch 1:  Current loss : 0.698144793510437 Validation Accuracy: 0.7009998559951782\n",
      "Epoch 87, CIFAR-10 Batch 2:  Current loss : 0.6235312819480896 Validation Accuracy: 0.7063998579978943\n",
      "Epoch 87, CIFAR-10 Batch 3:  Current loss : 0.5149868130683899 Validation Accuracy: 0.6953998804092407\n",
      "Epoch 87, CIFAR-10 Batch 4:  Current loss : 0.6178901195526123 Validation Accuracy: 0.6973998546600342\n",
      "Epoch 87, CIFAR-10 Batch 5:  Current loss : 0.6556953191757202 Validation Accuracy: 0.6981998085975647\n",
      "Epoch 88, CIFAR-10 Batch 1:  Current loss : 0.7189005613327026 Validation Accuracy: 0.6915999054908752\n",
      "Epoch 88, CIFAR-10 Batch 2:  Current loss : 0.6070088744163513 Validation Accuracy: 0.7053999304771423\n",
      "Epoch 88, CIFAR-10 Batch 3:  Current loss : 0.513262927532196 Validation Accuracy: 0.6933998465538025\n",
      "Epoch 88, CIFAR-10 Batch 4:  Current loss : 0.6150599718093872 Validation Accuracy: 0.7093998789787292\n",
      "Epoch 88, CIFAR-10 Batch 5:  Current loss : 0.6370126008987427 Validation Accuracy: 0.7025998830795288\n",
      "Epoch 89, CIFAR-10 Batch 1:  Current loss : 0.7308202385902405 Validation Accuracy: 0.6951998472213745\n",
      "Epoch 89, CIFAR-10 Batch 2:  Current loss : 0.6169796586036682 Validation Accuracy: 0.7061998248100281\n",
      "Epoch 89, CIFAR-10 Batch 3:  Current loss : 0.48980289697647095 Validation Accuracy: 0.6991998553276062\n",
      "Epoch 89, CIFAR-10 Batch 4:  Current loss : 0.6392247080802917 Validation Accuracy: 0.7025998830795288\n",
      "Epoch 89, CIFAR-10 Batch 5:  Current loss : 0.6623476147651672 Validation Accuracy: 0.6981998682022095\n",
      "Epoch 90, CIFAR-10 Batch 1:  Current loss : 0.7098875641822815 Validation Accuracy: 0.6999999284744263\n",
      "Epoch 90, CIFAR-10 Batch 2:  Current loss : 0.6172372698783875 Validation Accuracy: 0.703799843788147\n",
      "Epoch 90, CIFAR-10 Batch 3:  Current loss : 0.4952930510044098 Validation Accuracy: 0.6939998865127563\n",
      "Epoch 90, CIFAR-10 Batch 4:  Current loss : 0.6218746900558472 Validation Accuracy: 0.703799843788147\n",
      "Epoch 90, CIFAR-10 Batch 5:  Current loss : 0.6385005712509155 Validation Accuracy: 0.705599844455719\n",
      "Epoch 91, CIFAR-10 Batch 1:  Current loss : 0.7013004422187805 Validation Accuracy: 0.7029998302459717\n",
      "Epoch 91, CIFAR-10 Batch 2:  Current loss : 0.6171879768371582 Validation Accuracy: 0.7027997970581055\n",
      "Epoch 91, CIFAR-10 Batch 3:  Current loss : 0.4881971478462219 Validation Accuracy: 0.7031998634338379\n",
      "Epoch 91, CIFAR-10 Batch 4:  Current loss : 0.6070951819419861 Validation Accuracy: 0.703799843788147\n",
      "Epoch 91, CIFAR-10 Batch 5:  Current loss : 0.6660172939300537 Validation Accuracy: 0.6987998485565186\n",
      "Epoch 92, CIFAR-10 Batch 1:  Current loss : 0.7109088897705078 Validation Accuracy: 0.6953998804092407\n",
      "Epoch 92, CIFAR-10 Batch 2:  Current loss : 0.5995515584945679 Validation Accuracy: 0.702599823474884\n",
      "Epoch 92, CIFAR-10 Batch 3:  Current loss : 0.5068671107292175 Validation Accuracy: 0.6957998275756836\n",
      "Epoch 92, CIFAR-10 Batch 4:  Current loss : 0.615451455116272 Validation Accuracy: 0.6985997557640076\n",
      "Epoch 92, CIFAR-10 Batch 5:  Current loss : 0.6530064344406128 Validation Accuracy: 0.7075998783111572\n",
      "Epoch 93, CIFAR-10 Batch 1:  Current loss : 0.7223914861679077 Validation Accuracy: 0.6985998749732971\n",
      "Epoch 93, CIFAR-10 Batch 2:  Current loss : 0.6045302152633667 Validation Accuracy: 0.7029998302459717\n",
      "Epoch 93, CIFAR-10 Batch 3:  Current loss : 0.5079203248023987 Validation Accuracy: 0.6969998478889465\n",
      "Epoch 93, CIFAR-10 Batch 4:  Current loss : 0.6178045272827148 Validation Accuracy: 0.7023998498916626\n",
      "Epoch 93, CIFAR-10 Batch 5:  Current loss : 0.6450576782226562 Validation Accuracy: 0.705599844455719\n",
      "Epoch 94, CIFAR-10 Batch 1:  Current loss : 0.7028319239616394 Validation Accuracy: 0.6941998600959778\n",
      "Epoch 94, CIFAR-10 Batch 2:  Current loss : 0.5879367589950562 Validation Accuracy: 0.7047998905181885\n",
      "Epoch 94, CIFAR-10 Batch 3:  Current loss : 0.484899640083313 Validation Accuracy: 0.6965999007225037\n",
      "Epoch 94, CIFAR-10 Batch 4:  Current loss : 0.6210778951644897 Validation Accuracy: 0.70579993724823\n",
      "Epoch 94, CIFAR-10 Batch 5:  Current loss : 0.6442102789878845 Validation Accuracy: 0.709199845790863\n",
      "Epoch 95, CIFAR-10 Batch 1:  Current loss : 0.6951292753219604 Validation Accuracy: 0.6965998411178589\n",
      "Epoch 95, CIFAR-10 Batch 2:  Current loss : 0.5894842743873596 Validation Accuracy: 0.7029998898506165\n",
      "Epoch 95, CIFAR-10 Batch 3:  Current loss : 0.48872488737106323 Validation Accuracy: 0.700799822807312\n",
      "Epoch 95, CIFAR-10 Batch 4:  Current loss : 0.6032423973083496 Validation Accuracy: 0.7065998315811157\n",
      "Epoch 95, CIFAR-10 Batch 5:  Current loss : 0.6622084379196167 Validation Accuracy: 0.6963998079299927\n",
      "Epoch 96, CIFAR-10 Batch 1:  Current loss : 0.7269689440727234 Validation Accuracy: 0.6925998330116272\n",
      "Epoch 96, CIFAR-10 Batch 2:  Current loss : 0.604439377784729 Validation Accuracy: 0.7063999176025391\n",
      "Epoch 96, CIFAR-10 Batch 3:  Current loss : 0.5025897026062012 Validation Accuracy: 0.6933998465538025\n",
      "Epoch 96, CIFAR-10 Batch 4:  Current loss : 0.6131882667541504 Validation Accuracy: 0.7013998627662659\n",
      "Epoch 96, CIFAR-10 Batch 5:  Current loss : 0.6585732698440552 Validation Accuracy: 0.6935998797416687\n",
      "Epoch 97, CIFAR-10 Batch 1:  Current loss : 0.7250076532363892 Validation Accuracy: 0.6963998675346375\n",
      "Epoch 97, CIFAR-10 Batch 2:  Current loss : 0.5980010032653809 Validation Accuracy: 0.7005999088287354\n",
      "Epoch 97, CIFAR-10 Batch 3:  Current loss : 0.4982554316520691 Validation Accuracy: 0.6945998668670654\n",
      "Epoch 97, CIFAR-10 Batch 4:  Current loss : 0.6123923063278198 Validation Accuracy: 0.700799822807312\n",
      "Epoch 97, CIFAR-10 Batch 5:  Current loss : 0.6657126545906067 Validation Accuracy: 0.6975998878479004\n",
      "Epoch 98, CIFAR-10 Batch 1:  Current loss : 0.6985070705413818 Validation Accuracy: 0.7033998966217041\n",
      "Epoch 98, CIFAR-10 Batch 2:  Current loss : 0.5878209471702576 Validation Accuracy: 0.7035999298095703\n",
      "Epoch 98, CIFAR-10 Batch 3:  Current loss : 0.4836087226867676 Validation Accuracy: 0.6979998350143433\n",
      "Epoch 98, CIFAR-10 Batch 4:  Current loss : 0.5989862084388733 Validation Accuracy: 0.7075998783111572\n",
      "Epoch 98, CIFAR-10 Batch 5:  Current loss : 0.6431298851966858 Validation Accuracy: 0.7013998627662659\n",
      "Epoch 99, CIFAR-10 Batch 1:  Current loss : 0.7050133347511292 Validation Accuracy: 0.6985998749732971\n",
      "Epoch 99, CIFAR-10 Batch 2:  Current loss : 0.5770039558410645 Validation Accuracy: 0.7081998586654663\n",
      "Epoch 99, CIFAR-10 Batch 3:  Current loss : 0.47998207807540894 Validation Accuracy: 0.6981998682022095\n",
      "Epoch 99, CIFAR-10 Batch 4:  Current loss : 0.6020467281341553 Validation Accuracy: 0.7099998593330383\n",
      "Epoch 99, CIFAR-10 Batch 5:  Current loss : 0.644353985786438 Validation Accuracy: 0.6987999081611633\n",
      "Epoch 100, CIFAR-10 Batch 1:  Current loss : 0.7252565622329712 Validation Accuracy: 0.6995998024940491\n",
      "Epoch 100, CIFAR-10 Batch 2:  Current loss : 0.5763905644416809 Validation Accuracy: 0.7065998315811157\n",
      "Epoch 100, CIFAR-10 Batch 3:  Current loss : 0.49475258588790894 Validation Accuracy: 0.6989998817443848\n",
      "Epoch 100, CIFAR-10 Batch 4:  Current loss : 0.6062710285186768 Validation Accuracy: 0.7045998573303223\n",
      "Epoch 100, CIFAR-10 Batch 5:  Current loss : 0.6478626728057861 Validation Accuracy: 0.6951999068260193\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6905999898910522\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npWX3GqJHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3e4c670d30>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
